{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained w2v model\n",
    "def get_w2v_model(filename):\n",
    "    model = Word2Vec.load(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "def rm_spec(sent):\n",
    "    ret = re.sub('[\\\\n\\s+\\.\\!\\/_,$%^*(+\\\"\\')]+|[+——\\-()?【】《》“”！，。？、~@#￥%……&*（）]+', '', sent)\n",
    "    if ret:\n",
    "        return ret\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentence\n",
    "def split_sent(sent):\n",
    "    sents = re.split('[。？！\\n]', sent)\n",
    "    ret = []\n",
    "    for s in sents:\n",
    "        sl = jieba.lcut(s)\n",
    "        slt = []\n",
    "        for item in sl:\n",
    "            sr = rm_spec(item)\n",
    "            if sr:\n",
    "                slt.append(sr)\n",
    "        if slt:\n",
    "            ret.append(' '.join(slt))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process news\n",
    "def process_news_corpus(filename, news, lst):\n",
    "    di = defaultdict(list)\n",
    "    xna_lst = []\n",
    "    count = 0\n",
    "    with open(filename, 'w', encoding='utf-8') as fout:\n",
    "        for i, item in enumerate(lst):\n",
    "            sent = news['content'][item]\n",
    "            sents = split_sent(sent)\n",
    "            if sents:\n",
    "                di[i].append(count)\n",
    "                count += len(sents)\n",
    "                di[i].append(count)\n",
    "                if '新华社' in news['source'][item] or '新华网' in news['source'][item]:\n",
    "                    xna_lst.append(i)\n",
    "                fout.write('\\n'.join(sents) + '\\n')\n",
    "    return di, xna_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new corpus to train w2v model\n",
    "def add_more_train_corpus(model, filename):\n",
    "    sen_list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            sen_list.append(line.split())\n",
    "    model.train(sentences=sen_list, total_examples=len(sen_list), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get news sentences index list from a dict structure\n",
    "def get_index_lst_from_dict(di, lst):\n",
    "    ret = []\n",
    "    for i in lst:\n",
    "        start = di[i][0]\n",
    "        end = di[i][1]\n",
    "        for t in range(start, end):\n",
    "            ret.append(t)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_lst_from_dict(di, idx):\n",
    "    ret = []\n",
    "    start = di[idx][0]\n",
    "    end = di[idx][1]\n",
    "    for t in range(start, end):\n",
    "        ret.append(t)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word frequency\n",
    "def word_freq(corpus_file):\n",
    "    word_list = []\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            word_list += line.split()\n",
    "    cc = Counter(word_list)\n",
    "    num_all = sum(cc.values())\n",
    "\n",
    "    def get_word_freq(word):\n",
    "        return cc[word] / num_all\n",
    "\n",
    "    return get_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence vector matrix\n",
    "def get_sent_vec(model, get_wd_freq, corpus_file):\n",
    "    a = 0.001\n",
    "    col = model.wv.vector_size\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as fin:\n",
    "        all_lines = fin.readlines()\n",
    "        ret = np.zeros((len(all_lines), col))\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            sent_vec = np.zeros(col)\n",
    "            wd_lst = line.split()\n",
    "            for wd in wd_lst:\n",
    "                try:\n",
    "                    pw = get_wd_freq(wd)\n",
    "                    w = a / (a + pw)\n",
    "                    sent_vec += w * np.array(model.wv[wd])\n",
    "                except:\n",
    "                    pass\n",
    "            ret[idx] = sent_vec\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_with_content(news):\n",
    "    ret = []\n",
    "    for i in range(len(news)):\n",
    "        if pd.isna(news['source'][i]) or pd.isna(news['content'][i]):\n",
    "            continue\n",
    "        ret.append(i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remain_list(all_samples, part_samples):\n",
    "    ret = []\n",
    "    for i in all_samples:\n",
    "        if i not in part_samples:\n",
    "            ret.append(i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model's precision and recall rate\n",
    "def get_precision_and_recall(xna_test_res, otr_test_res):\n",
    "    cc1 = Counter(xna_test_res)\n",
    "    cc2 = Counter(otr_test_res)\n",
    "    tp = cc1[1]\n",
    "    fp = cc2[1]\n",
    "    tn = cc2[0]\n",
    "    fn = cc1[0]\n",
    "    preci = tp / (tp + fp)\n",
    "    recal = tp / (tp + fn)\n",
    "    return preci, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNClassifier(xTrain, yTrain):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "    neigh.fit(xTrain, yTrain)\n",
    "    return neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassifier(xTrain, yTrain):\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTClassifier(xTrain, yTrain):\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFClassifier(xTrain, yTrain):\n",
    "    clf = RandomForestClassifier(n_estimators=10, max_depth=10)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifier:\n",
    "    def __init__(self, xTrain, yTrain):\n",
    "        dtrain = xgb.DMatrix(xTrain, label=yTrain)\n",
    "        params = {'objective': 'binary:logitraw', 'silent': 1, 'n_estimators': 1000,\n",
    "                  'max_depth': 8}\n",
    "        self.clf = xgb.train(params, dtrain)\n",
    "\n",
    "    def predict(self, mat):\n",
    "        test = xgb.DMatrix(mat)\n",
    "        res = self.clf.predict(test)\n",
    "        return res.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify main process\n",
    "def main_func(classifier, mat, idx_dict, xna_trn_lst, otr_trn_lst, xna_test_lst, otr_test_lst):\n",
    "    xna_trn_sent_lst = get_index_lst_from_dict(idx_dict, xna_trn_lst)\n",
    "    otr_trn_sent_lst = get_index_lst_from_dict(idx_dict, otr_trn_lst)\n",
    "    print('XNA news sentences for training:', len(xna_trn_sent_lst))\n",
    "    print('Other news sentences for training:', len(otr_trn_sent_lst))\n",
    "\n",
    "    X_train = mat[xna_trn_sent_lst + otr_trn_sent_lst, :]\n",
    "    Y = np.array([0] * mat.shape[0])\n",
    "    for i in xna_trn_sent_lst: Y[i] = 1\n",
    "    y_train = Y[xna_trn_sent_lst + otr_trn_sent_lst]\n",
    "\n",
    "    trainedModel = classifier(X_train, y_train)\n",
    "\n",
    "    xna_test = []\n",
    "    otr_test = []\n",
    "    threshold = 0.3  # Similiar sentence amount over 30% is classified as plagiarized\n",
    "\n",
    "    for xt in xna_test_lst:\n",
    "        sent_lst = get_idx_lst_from_dict(idx_dict, xt)\n",
    "        scores = []\n",
    "        for si in sent_lst:\n",
    "            sco = trainedModel.predict([mat[si]])[0]\n",
    "            scores.append(sco)\n",
    "        cc = Counter(scores)\n",
    "        if cc[1] / len(sent_lst) > threshold:\n",
    "            xna_test.append(1)\n",
    "        else:\n",
    "            xna_test.append(0)\n",
    "\n",
    "    for ot in otr_test_lst:\n",
    "        sent_lst = get_idx_lst_from_dict(idx_dict, ot)\n",
    "        scores = []\n",
    "        for si in sent_lst:\n",
    "            sco = trainedModel.predict([mat[si]])[0]\n",
    "            scores.append(sco)\n",
    "        cc = Counter(scores)\n",
    "        if cc[1] / len(sent_lst) > threshold:\n",
    "            otr_test.append(1)\n",
    "        else:\n",
    "            otr_test.append(0)\n",
    "\n",
    "    precision, recall = get_precision_and_recall(xna_test, otr_test)\n",
    "    print(classifier.__name__ + ' precision {}, recall {}'.format(precision, recall))\n",
    "\n",
    "    # lst2file('trn_result.txt', xna_test)\n",
    "    # lst2file('otr_result.txt', otr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import original corpus\n",
    "news_df = pd.read_csv('sqlResult_1558435.csv', encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get news list which has source and content\n",
    "lst_with_content = get_list_with_content(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\King\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.678 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Build news index dict of corpus file, and filter XNA news\n",
    "index_dict, xna_news_lst = process_news_corpus('news_corpus.txt', news_df, lst_with_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "otr_news_lst = list(set(index_dict.keys()) - set(xna_news_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_n = 1000\n",
    "sample_test_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "xna_samples_train = random.sample(xna_news_lst, sample_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xna_samples_test = random.sample(set(xna_news_lst) - set(xna_samples_train), sample_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "otr_samples_train = random.sample(otr_news_lst, sample_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "otr_samples_test = random.sample(set(otr_news_lst) - set(otr_samples_train), sample_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = get_w2v_model('wiki_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_more_train_corpus(w2v_model, 'news_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_prob = word_freq('news_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent_mat = get_sent_vec(w2v_model, get_word_prob, 'news_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 7610\n",
      "Other news sentences for training: 22870\n",
      "KNNClassifier precision 0.9523809523809523, recall 0.8\n"
     ]
    }
   ],
   "source": [
    "## KNN\n",
    "main_func(KNNClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 7610\n",
      "Other news sentences for training: 22870\n",
      "DTClassifier precision 0.7203389830508474, recall 0.85\n"
     ]
    }
   ],
   "source": [
    "## Decision tree\n",
    "main_func(DTClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 7610\n",
      "Other news sentences for training: 22870\n",
      "RFClassifier precision 0.9444444444444444, recall 0.68\n"
     ]
    }
   ],
   "source": [
    "## Random forest\n",
    "main_func(RFClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 7610\n",
      "Other news sentences for training: 22870\n",
      "XGBClassifier precision 1.0, recall 0.27\n"
     ]
    }
   ],
   "source": [
    "## XGBoost\n",
    "main_func(XGBClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 6724\n",
      "Other news sentences for training: 23659\n",
      "SVMClassifier precision 1.0, recall 0.44\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "main_func(SVMClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
