{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained w2v model\n",
    "def get_w2v_model(filename):\n",
    "    model = Word2Vec.load(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "def rm_spec(sent):\n",
    "    ret = re.sub('[\\\\n\\s+\\.\\!\\/_,$%^*(+\\\"\\')]+|[+——\\-()?【】《》“”！，。？、~@#￥%……&*（）]+', '', sent)\n",
    "    if ret:\n",
    "        return ret\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentence\n",
    "def split_sent(sent):\n",
    "    sents = re.split('[。？！\\n]', sent)\n",
    "    ret = []\n",
    "    for s in sents:\n",
    "        sl = jieba.lcut(s)\n",
    "        slt = []\n",
    "        for item in sl:\n",
    "            sr = rm_spec(item)\n",
    "            if sr:\n",
    "                slt.append(sr)\n",
    "        if slt:\n",
    "            ret.append(' '.join(slt))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process news\n",
    "def process_news_corpus(filename, news, lst):\n",
    "    di = defaultdict(list)\n",
    "    xna_lst = []\n",
    "    count = 0\n",
    "    with open(filename, 'w', encoding='utf-8') as fout:\n",
    "        for i, item in enumerate(lst):\n",
    "            sent = news['content'][item]\n",
    "            sent = re.sub('\\\\n', '', sent)\n",
    "            sents = split_sent(sent)\n",
    "            if sents:\n",
    "                di[i].append(count)\n",
    "                count += len(sents)\n",
    "                di[i].append(count)\n",
    "                if '新华社' in news['source'][item] or '新华网' in news['source'][item]:\n",
    "                    xna_lst.append(i)\n",
    "                fout.write('\\n'.join(sents) + '\\n')\n",
    "    return di, xna_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new corpus to train w2v model\n",
    "def add_more_train_corpus(model, filename):\n",
    "    sen_list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            sen_list.append(line.split())\n",
    "    model.train(sentences=sen_list, total_examples=len(sen_list), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get news sentences index list from a dict structure\n",
    "def get_index_lst_from_dict(di, lst):\n",
    "    ret = []\n",
    "    for i in lst:\n",
    "        start = di[i][0]\n",
    "        end = di[i][1]\n",
    "        for t in range(start, end):\n",
    "            ret.append(t)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_lst_from_dict(di, idx):\n",
    "    ret = []\n",
    "    start = di[idx][0]\n",
    "    end = di[idx][1]\n",
    "    for t in range(start, end):\n",
    "        ret.append(t)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word frequency\n",
    "def word_freq(corpus_file):\n",
    "    word_list = []\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin.readlines():\n",
    "            word_list += line.split()\n",
    "    cc = Counter(word_list)\n",
    "    num_all = sum(cc.values())\n",
    "\n",
    "    def get_word_freq(word):\n",
    "        return cc[word] / num_all\n",
    "\n",
    "    return get_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence vector matrix\n",
    "def get_sent_vec(model, get_wd_freq, corpus_file):\n",
    "    a = 0.001\n",
    "    col = model.wv.vector_size\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as fin:\n",
    "        all_lines = fin.readlines()\n",
    "        ret = np.zeros((len(all_lines), col))\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            sent_vec = np.zeros(col)\n",
    "            wd_lst = line.split()\n",
    "            for wd in wd_lst:\n",
    "                try:\n",
    "                    pw = get_wd_freq(wd)\n",
    "                    w = a / (a + pw)\n",
    "                    sent_vec += w * np.array(model.wv[wd])\n",
    "                except:\n",
    "                    pass\n",
    "            ret[idx] = sent_vec\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_with_content(news):\n",
    "    ret = []\n",
    "    for i in range(len(news)):\n",
    "        if pd.isna(news['source'][i]) or pd.isna(news['content'][i]):\n",
    "            continue\n",
    "        ret.append(i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remain_list(all_samples, part_samples):\n",
    "    ret = []\n",
    "    for i in all_samples:\n",
    "        if i not in part_samples:\n",
    "            ret.append(i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model's precision and recall rate\n",
    "def get_precision_and_recall(xna_test_res, otr_test_res):\n",
    "    cc1 = Counter(xna_test_res)\n",
    "    cc2 = Counter(otr_test_res)\n",
    "    tp = cc1[1]\n",
    "    fp = cc2[1]\n",
    "    tn = cc2[0]\n",
    "    fn = cc1[0]\n",
    "    preci = tp / (tp + fp)\n",
    "    recal = tp / (tp + fn)\n",
    "    return preci, recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNClassifier(xTrain, yTrain):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "    neigh.fit(xTrain, yTrain)\n",
    "    return neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassifier(xTrain, yTrain):\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTClassifier(xTrain, yTrain):\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFClassifier(xTrain, yTrain):\n",
    "    clf = RandomForestClassifier(n_estimators=10, max_depth=10)\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifier:\n",
    "    def __init__(self, xTrain, yTrain, xVali, yVali):\n",
    "        dtrain = xgb.DMatrix(xTrain, label=yTrain)\n",
    "        dVali = xgb.DMatrix(xVali, label=yVali)\n",
    "        params = {'objective': 'binary:logistic', 'silent': 1, 'nthread': 4,\n",
    "                  'max_depth': 3, 'eta': 1.2, 'min_child_weight': 2}\n",
    "        evallist = [(dVali, 'eval'), (dtrain, 'train')]\n",
    "        self.clf = xgb.train(params, dtrain, num_boost_round=100, evals=evallist)\n",
    "\n",
    "    def predict(self, mat):\n",
    "        test = xgb.DMatrix(mat)\n",
    "        res = self.clf.predict(test)\n",
    "        res = (res >= 0.5) * 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify main process\n",
    "def main_func(classifier, mat, idx_dict, xna_trn_lst, otr_trn_lst, xna_test_lst, otr_test_lst):\n",
    "    xna_trn_sent_lst = get_index_lst_from_dict(idx_dict, xna_trn_lst)\n",
    "    otr_trn_sent_lst = get_index_lst_from_dict(idx_dict, otr_trn_lst)\n",
    "    print('XNA news sentences for training:', len(xna_trn_sent_lst))\n",
    "    print('Other news sentences for training:', len(otr_trn_sent_lst))\n",
    "\n",
    "    X_train = mat[xna_trn_sent_lst + otr_trn_sent_lst, :]\n",
    "    Y = np.array([0] * mat.shape[0])\n",
    "    for i in xna_trn_sent_lst: Y[i] = 1\n",
    "    y_train = Y[xna_trn_sent_lst + otr_trn_sent_lst]\n",
    "\n",
    "    trainedModel = classifier(X_train, y_train)\n",
    "\n",
    "    xna_test = []\n",
    "    otr_test = []\n",
    "    threshold = 0.3  # Similiar sentence amount over 30% is classified as plagiarized\n",
    "\n",
    "    for xt in xna_test_lst:\n",
    "        sent_lst = get_idx_lst_from_dict(idx_dict, xt)\n",
    "        scores = []\n",
    "        for si in sent_lst:\n",
    "            sco = trainedModel.predict([mat[si]])[0]\n",
    "            scores.append(sco)\n",
    "        cc = Counter(scores)\n",
    "        if cc[1] / len(sent_lst) > threshold:\n",
    "            xna_test.append(1)\n",
    "        else:\n",
    "            xna_test.append(0)\n",
    "\n",
    "    for ot in otr_test_lst:\n",
    "        sent_lst = get_idx_lst_from_dict(idx_dict, ot)\n",
    "        scores = []\n",
    "        for si in sent_lst:\n",
    "            sco = trainedModel.predict([mat[si]])[0]\n",
    "            scores.append(sco)\n",
    "        cc = Counter(scores)\n",
    "        if cc[1] / len(sent_lst) > threshold:\n",
    "            otr_test.append(1)\n",
    "        else:\n",
    "            otr_test.append(0)\n",
    "\n",
    "    precision, recall = get_precision_and_recall(xna_test, otr_test)\n",
    "    print(classifier.__name__ + ' precision {}, recall {}'.format(precision, recall))\n",
    "\n",
    "    # lst2file('trn_result.txt', xna_test)\n",
    "    # lst2file('otr_result.txt', otr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_func(classifier, mat, idx_dict, xna_trn_lst, otr_trn_lst, xna_test_lst, otr_test_lst,\n",
    "             xna_vali_lst, otr_vali_lst):\n",
    "    xna_trn_sent_lst = get_index_lst_from_dict(idx_dict, xna_trn_lst)\n",
    "    otr_trn_sent_lst = get_index_lst_from_dict(idx_dict, otr_trn_lst)\n",
    "    # print('XNA news sentences for training:', len(xna_trn_sent_lst))\n",
    "    # print('Other news sentences for training:', len(otr_trn_sent_lst))\n",
    "\n",
    "    X_train = mat[xna_trn_sent_lst + otr_trn_sent_lst, :]\n",
    "    Y = np.array([0] * mat.shape[0])\n",
    "    for i in xna_trn_sent_lst: Y[i] = 1\n",
    "    y_train = Y[xna_trn_sent_lst + otr_trn_sent_lst]\n",
    "\n",
    "    xna_vali_sent_lst = get_index_lst_from_dict(idx_dict, xna_vali_lst)\n",
    "    otr_vali_sent_lst = get_index_lst_from_dict(idx_dict, otr_vali_lst)\n",
    "    X_vali = mat[xna_vali_sent_lst + otr_vali_sent_lst, :]\n",
    "    Y_vali = np.array([0] * mat.shape[0])\n",
    "    for i in xna_vali_sent_lst: Y_vali[i] = 1\n",
    "    y_vali = Y_vali[xna_vali_sent_lst + otr_vali_sent_lst]\n",
    "\n",
    "    trainedModel = classifier(X_train, y_train, X_vali, y_vali)\n",
    "\n",
    "    xna_test = []\n",
    "    otr_test = []\n",
    "    threshold = 0.3  # Similiar sentence amount over 30% is classified as plagiarized\n",
    "\n",
    "    for xt in xna_test_lst:\n",
    "        sent_lst = get_idx_lst_from_dict(idx_dict, xt)\n",
    "        scores = []\n",
    "        for si in sent_lst:\n",
    "            sco = trainedModel.predict([mat[si]])[0]\n",
    "            scores.append(sco)\n",
    "        cc = Counter(scores)\n",
    "        if cc[1] / len(sent_lst) > threshold:\n",
    "            xna_test.append(1)\n",
    "        else:\n",
    "            xna_test.append(0)\n",
    "\n",
    "    for ot in otr_test_lst:\n",
    "        sent_lst = get_idx_lst_from_dict(idx_dict, ot)\n",
    "        scores = []\n",
    "        for si in sent_lst:\n",
    "            sco = trainedModel.predict([mat[si]])[0]\n",
    "            scores.append(sco)\n",
    "        cc = Counter(scores)\n",
    "        if cc[1] / len(sent_lst) > threshold:\n",
    "            otr_test.append(1)\n",
    "        else:\n",
    "            otr_test.append(0)\n",
    "\n",
    "    precision, recall = get_precision_and_recall(xna_test, otr_test)\n",
    "    print(classifier.__name__ + ' precision {}, recall {}'.format(precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import original corpus\n",
    "news_df = pd.read_csv('sqlResult_1558435.csv', encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get news list which has source and content\n",
    "lst_with_content = get_list_with_content(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build news index dict of corpus file, and filter XNA news\n",
    "index_dict, xna_news_lst = process_news_corpus('news_corpus.txt', news_df, lst_with_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "otr_news_lst = list(set(index_dict.keys()) - set(xna_news_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_n = 2000\n",
    "sample_test_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "xna_samples_train = random.sample(xna_news_lst, sample_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "xna_samples_test = random.sample(set(xna_news_lst) - set(xna_samples_train), sample_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "otr_samples_train = random.sample(otr_news_lst, sample_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "otr_samples_test = random.sample(set(otr_news_lst) - set(otr_samples_train), sample_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = get_w2v_model('wiki_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_more_train_corpus(w2v_model, 'news_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_prob = word_freq('news_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent_mat = get_sent_vec(w2v_model, get_word_prob, 'news_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 14482\n",
      "Other news sentences for training: 38341\n",
      "KNNClassifier precision 0.9281437125748503, recall 0.775\n"
     ]
    }
   ],
   "source": [
    "## KNN\n",
    "main_func(KNNClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 14482\n",
      "Other news sentences for training: 38341\n",
      "DTClassifier precision 0.776824034334764, recall 0.905\n"
     ]
    }
   ],
   "source": [
    "## Decision tree\n",
    "main_func(DTClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 14482\n",
      "Other news sentences for training: 38341\n",
      "RFClassifier precision 0.9565217391304348, recall 0.66\n"
     ]
    }
   ],
   "source": [
    "## Random forest\n",
    "main_func(RFClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "xna_samples_vali = random.sample(set(xna_news_lst) - set(xna_samples_train) - set(xna_samples_test),\n",
    "                                 sample_test_n)\n",
    "otr_samples_vali = random.sample(set(otr_news_lst) - set(otr_samples_train) - set(xna_samples_test),\n",
    "                                 sample_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.232624\ttrain-error:0.228461\n",
      "[1]\teval-error:0.236474\ttrain-error:0.220472\n",
      "[2]\teval-error:0.229179\ttrain-error:0.213335\n",
      "[3]\teval-error:0.228571\ttrain-error:0.211575\n",
      "[4]\teval-error:0.224519\ttrain-error:0.210817\n",
      "[5]\teval-error:0.224316\ttrain-error:0.207561\n",
      "[6]\teval-error:0.224721\ttrain-error:0.205725\n",
      "[7]\teval-error:0.223506\ttrain-error:0.203491\n",
      "[8]\teval-error:0.223911\ttrain-error:0.201636\n",
      "[9]\teval-error:0.222898\ttrain-error:0.200897\n",
      "[10]\teval-error:0.221885\ttrain-error:0.199421\n",
      "[11]\teval-error:0.221885\ttrain-error:0.197812\n",
      "[12]\teval-error:0.223303\ttrain-error:0.196373\n",
      "[13]\teval-error:0.221885\ttrain-error:0.195313\n",
      "[14]\teval-error:0.22229\ttrain-error:0.193457\n",
      "[15]\teval-error:0.223506\ttrain-error:0.19306\n",
      "[16]\teval-error:0.220263\ttrain-error:0.192511\n",
      "[17]\teval-error:0.220669\ttrain-error:0.190637\n",
      "[18]\teval-error:0.220871\ttrain-error:0.190182\n",
      "[19]\teval-error:0.221277\ttrain-error:0.190012\n",
      "[20]\teval-error:0.220871\ttrain-error:0.18827\n",
      "[21]\teval-error:0.221479\ttrain-error:0.18827\n",
      "[22]\teval-error:0.220263\ttrain-error:0.188232\n",
      "[23]\teval-error:0.220466\ttrain-error:0.186737\n",
      "[24]\teval-error:0.220263\ttrain-error:0.185487\n",
      "[25]\teval-error:0.221074\ttrain-error:0.184162\n",
      "[26]\teval-error:0.217224\ttrain-error:0.184068\n",
      "[27]\teval-error:0.221074\ttrain-error:0.182421\n",
      "[28]\teval-error:0.219048\ttrain-error:0.181474\n",
      "[29]\teval-error:0.217021\ttrain-error:0.18119\n",
      "[30]\teval-error:0.217021\ttrain-error:0.180982\n",
      "[31]\teval-error:0.219048\ttrain-error:0.179751\n",
      "[32]\teval-error:0.217224\ttrain-error:0.178994\n",
      "[33]\teval-error:0.216616\ttrain-error:0.178388\n",
      "[34]\teval-error:0.212563\ttrain-error:0.178275\n",
      "[35]\teval-error:0.213576\ttrain-error:0.178142\n",
      "[36]\teval-error:0.212766\ttrain-error:0.177612\n",
      "[37]\teval-error:0.21459\ttrain-error:0.176438\n",
      "[38]\teval-error:0.21459\ttrain-error:0.175227\n",
      "[39]\teval-error:0.215603\ttrain-error:0.174356\n",
      "[40]\teval-error:0.213982\ttrain-error:0.174337\n",
      "[41]\teval-error:0.213576\ttrain-error:0.17375\n",
      "[42]\teval-error:0.213576\ttrain-error:0.173561\n",
      "[43]\teval-error:0.214792\ttrain-error:0.173296\n",
      "[44]\teval-error:0.214792\ttrain-error:0.172179\n",
      "[45]\teval-error:0.213576\ttrain-error:0.171687\n",
      "[46]\teval-error:0.213779\ttrain-error:0.171232\n",
      "[47]\teval-error:0.2154\ttrain-error:0.171138\n",
      "[48]\teval-error:0.216413\ttrain-error:0.170816\n",
      "[49]\teval-error:0.216616\ttrain-error:0.170664\n",
      "[50]\teval-error:0.215603\ttrain-error:0.169604\n",
      "[51]\teval-error:0.21925\ttrain-error:0.169207\n",
      "[52]\teval-error:0.217629\ttrain-error:0.168449\n",
      "[53]\teval-error:0.216413\ttrain-error:0.168241\n",
      "[54]\teval-error:0.216211\ttrain-error:0.167484\n",
      "[55]\teval-error:0.214184\ttrain-error:0.167181\n",
      "[56]\teval-error:0.216413\ttrain-error:0.167124\n",
      "[57]\teval-error:0.217427\ttrain-error:0.166897\n",
      "[58]\teval-error:0.215805\ttrain-error:0.16614\n",
      "[59]\teval-error:0.216413\ttrain-error:0.164777\n",
      "[60]\teval-error:0.220061\ttrain-error:0.163963\n",
      "[61]\teval-error:0.219656\ttrain-error:0.16419\n",
      "[62]\teval-error:0.218845\ttrain-error:0.163622\n",
      "[63]\teval-error:0.217427\ttrain-error:0.162145\n",
      "[64]\teval-error:0.217427\ttrain-error:0.161445\n",
      "[65]\teval-error:0.220263\ttrain-error:0.161123\n",
      "[66]\teval-error:0.221277\ttrain-error:0.159817\n",
      "[67]\teval-error:0.221682\ttrain-error:0.160271\n",
      "[68]\teval-error:0.21844\ttrain-error:0.159741\n",
      "[69]\teval-error:0.219048\ttrain-error:0.15923\n",
      "[70]\teval-error:0.220466\ttrain-error:0.159419\n",
      "[71]\teval-error:0.219453\ttrain-error:0.158529\n",
      "[72]\teval-error:0.220669\ttrain-error:0.158302\n",
      "[73]\teval-error:0.218642\ttrain-error:0.158056\n",
      "[74]\teval-error:0.218034\ttrain-error:0.157772\n",
      "[75]\teval-error:0.217427\ttrain-error:0.156958\n",
      "[76]\teval-error:0.218237\ttrain-error:0.156012\n",
      "[77]\teval-error:0.218034\ttrain-error:0.155292\n",
      "[78]\teval-error:0.217427\ttrain-error:0.155538\n",
      "[79]\teval-error:0.218237\ttrain-error:0.155311\n",
      "[80]\teval-error:0.218237\ttrain-error:0.154989\n",
      "[81]\teval-error:0.217021\ttrain-error:0.15463\n",
      "[82]\teval-error:0.216616\ttrain-error:0.154213\n",
      "[83]\teval-error:0.217224\ttrain-error:0.154308\n",
      "[84]\teval-error:0.220263\ttrain-error:0.152964\n",
      "[85]\teval-error:0.21844\ttrain-error:0.152528\n",
      "[86]\teval-error:0.217427\ttrain-error:0.152377\n",
      "[87]\teval-error:0.217021\ttrain-error:0.152471\n",
      "[88]\teval-error:0.216819\ttrain-error:0.152282\n",
      "[89]\teval-error:0.217427\ttrain-error:0.151904\n",
      "[90]\teval-error:0.2154\ttrain-error:0.152169\n",
      "[91]\teval-error:0.218034\ttrain-error:0.15073\n",
      "[92]\teval-error:0.217832\ttrain-error:0.150597\n",
      "[93]\teval-error:0.219048\ttrain-error:0.150294\n",
      "[94]\teval-error:0.217427\ttrain-error:0.149726\n",
      "[95]\teval-error:0.21925\ttrain-error:0.148893\n",
      "[96]\teval-error:0.217224\ttrain-error:0.148344\n",
      "[97]\teval-error:0.219048\ttrain-error:0.148723\n",
      "[98]\teval-error:0.218034\ttrain-error:0.147833\n",
      "[99]\teval-error:0.219453\ttrain-error:0.147758\n",
      "XGBClassifier precision 0.8549222797927462, recall 0.825\n"
     ]
    }
   ],
   "source": [
    "## XGBoost\n",
    "xgb_func(XGBClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "         xna_samples_test, otr_samples_test, xna_samples_vali, otr_samples_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNA news sentences for training: 6724\n",
      "Other news sentences for training: 23659\n",
      "SVMClassifier precision 1.0, recall 0.44\n"
     ]
    }
   ],
   "source": [
    "## SVM, slow than others\n",
    "main_func(SVMClassifier, all_sent_mat, index_dict, xna_samples_train, otr_samples_train,\n",
    "          xna_samples_test, otr_samples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
